{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sparse_ternary_networks.Tools as T\n",
    "import sparse_ternary_networks.loadLab as load\n",
    "import numpy as np\n",
    "import sparse_ternary_networks.MLSTC as MLSTC\n",
    "import matplotlib.pyplot as plt\n",
    "##\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import sparse_ternary_networks.PyTorcher_MLSTC as STNetTorcher\n",
    "import RRQ.Tools as Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing the parameters:\n",
    "m = 500       # dimension of the codes\n",
    "k = 2         # number of non-zero elements of the STC at each stage.\n",
    "L = 5        # number of layer units of the network.\n",
    "N_train = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Torch hyper-params:\n",
    "dtype = torch.FloatTensor\n",
    "num_epoch = 10    \n",
    "learning_rate = 1e-3  \n",
    "num_batch = 100\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numRep = 1\n",
    "MethodList = ['STNetRandom','STNetMLSTC','Random','MLSTC','STNetProcrustean','Procrustean']\n",
    "for Method in MethodList:\n",
    "    exec('dist_' + Method  + '_train = np.zeros((numRep,L+1))')\n",
    "    exec('dist_' + Method  + '_test = np.zeros((numRep,L+1))')\n",
    "    #\n",
    "    exec('rate_' + Method  + ' = np.zeros((numRep,L+1))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist, fashion, cifar10 loading and normalization:\n",
    "path = '/home/sssohrab/Dropbox/Py/Data/mnist.npz'\n",
    "# path = '/home/sssohrab/Dropbox/Py/Data/cifar10.npz'\n",
    "data = np.load(path)\n",
    "if 'cifar10' in path:\n",
    "    nchannel = 3\n",
    "    Database = 'CIFAR10'\n",
    "elif 'mnist' in path:\n",
    "    nchannel = 1\n",
    "    Database = 'MNIST'\n",
    "elif 'fashion' in path:\n",
    "    nchannel = 1\n",
    "    Database = 'FASHION'\n",
    "data = np.load(path)\n",
    "imgs_train = np.divide(data['imgs_train'],255).astype(float)\n",
    "imgs_test = np.divide(data['imgs_test'],255).astype(float)\n",
    "# Y_train = data['Y_train'].reshape(1,-1)\n",
    "# Y_test = data['Y_test'].reshape(1,-1)\n",
    "# Normalization:\n",
    "imgs_train -= np.mean(imgs_train)\n",
    "imgs_test -= np.mean(imgs_test)\n",
    "#\n",
    "imgs_train /= np.std(imgs_train)\n",
    "imgs_test /= np.std(imgs_test)\n",
    "#imgs_resolution = (28,28,nchannel)\n",
    "#\n",
    "####################################################################################\n",
    "# DCT?:\n",
    "F_train = load.imgTensor2matrix_zigzagDCT(imgs_train,nchannel)\n",
    "F_test = load.imgTensor2matrix_zigzagDCT(imgs_test,nchannel)\n",
    "#F_train = imgs_train.reshape(-1,60000)\n",
    "#F_test = imgs_test.reshape(-1,10000)\n",
    "####################################################################################\n",
    "## Whitening using sub-band PCA:\n",
    "# numSB = 1\n",
    "# F0_train,EigVecs,dim_means = STNet.SubbandPCA_whitner(F0_train,numSB)\n",
    "# F0_test,_,_ = STNet.SubbandPCA_whitner(F0_test,numSB,EigVecs=EigVecs,dim_means=dim_means)\n",
    "##\n",
    "F_train = F_train[:,0:N_train]\n",
    "n,N_train = F_train.shape\n",
    "N_test = F_test.shape[1]\n",
    "#\n",
    "norm_train = np.divide(np.linalg.norm(F_train) ** 2, np.prod(F_train.shape))\n",
    "norm_test = np.divide(np.linalg.norm(F_test) ** 2, np.prod(F_test.shape))\n",
    "(n,_) = F_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # MNIST loading, normalization and pre-processing:\n",
    "# data = np.load('/home/sssohrab/Dropbox/Py/Data/mnist.npz')\n",
    "# imgs_train = np.divide(data['imgs_train'],255).astype(float)\n",
    "# imgs_test = np.divide(data['imgs_test'],255).astype(float)\n",
    "# # Y_train = data['Y_train']\n",
    "# # Y_test = data['Y_test']\n",
    "# # Normalization:\n",
    "# imgs_train -= np.mean(imgs_train)\n",
    "# imgs_test -= np.mean(imgs_test)\n",
    "# #\n",
    "# imgs_train /= np.std(imgs_train)\n",
    "# imgs_test /= np.std(imgs_test)\n",
    "# nchannel = 1\n",
    "# imgs_resolution = (28,28,nchannel)\n",
    "# #\n",
    "# ####################################################################################\n",
    "# # DCT?:\n",
    "# F_train = load.imgTensor2matrix_zigzagDCT(imgs_train,nchannel)\n",
    "# F_test = load.imgTensor2matrix_zigzagDCT(imgs_test,nchannel)\n",
    "# #F_train = imgs_train.reshape(-1,60000)\n",
    "# #F_test = imgs_test.reshape(-1,10000)\n",
    "# ############################\n",
    "# F_train = F_train[:,0:N_train]\n",
    "# n,N_train = F_train.shape\n",
    "# N_test = F_test.shape[1]\n",
    "# #\n",
    "# norm_train = np.divide(np.linalg.norm(F_train) ** 2, np.prod(F_train.shape))\n",
    "# norm_test = np.divide(np.linalg.norm(F_test) ** 2, np.prod(F_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................  Repetition =  1\n",
      " ############   STNetMLSTC     A L G O R I T H M   ################\n",
      "Pre-training:\n",
      " ################## Starting to learn network parameters:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished learning network parameters:\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished running the network.\n",
      "Fine-tuning: \n",
      "Epoch:  1 | train loss: 0.2466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sssohrab/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:73: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | train loss: 0.2381\n",
      "Epoch:  1 | train loss: 0.2366\n",
      "Epoch:  1 | train loss: 0.2397\n",
      "Epoch:  1 | train loss: 0.2381\n",
      "Epoch:  1 | train loss: 0.2344\n",
      "Epoch:  1 | train loss: 0.2384\n",
      "Epoch:  1 | train loss: 0.2308\n",
      "Epoch:  1 | train loss: 0.2254\n",
      "Epoch:  1 | train loss: 0.2325\n",
      "Epoch:  2 | train loss: 0.2272\n",
      "Epoch:  2 | train loss: 0.2280\n",
      "Epoch:  2 | train loss: 0.2276\n",
      "Epoch:  2 | train loss: 0.2223\n",
      "Epoch:  2 | train loss: 0.2096\n",
      "Epoch:  2 | train loss: 0.2185\n",
      "Epoch:  2 | train loss: 0.2177\n",
      "Epoch:  2 | train loss: 0.2181\n",
      "Epoch:  2 | train loss: 0.2081\n",
      "Epoch:  2 | train loss: 0.2139\n",
      "Epoch:  3 | train loss: 0.2031\n",
      "Epoch:  3 | train loss: 0.2067\n",
      "Epoch:  3 | train loss: 0.2110\n",
      "Epoch:  3 | train loss: 0.2017\n",
      "Epoch:  3 | train loss: 0.1992\n",
      "Epoch:  3 | train loss: 0.2056\n",
      "Epoch:  3 | train loss: 0.1879\n",
      "Epoch:  3 | train loss: 0.1938\n",
      "Epoch:  3 | train loss: 0.1895\n",
      "Epoch:  3 | train loss: 0.1960\n",
      "Epoch:  4 | train loss: 0.1877\n",
      "Epoch:  4 | train loss: 0.1847\n",
      "Epoch:  4 | train loss: 0.1858\n",
      "Epoch:  4 | train loss: 0.1867\n",
      "Epoch:  4 | train loss: 0.1813\n",
      "Epoch:  4 | train loss: 0.1792\n",
      "Epoch:  4 | train loss: 0.1763\n",
      "Epoch:  4 | train loss: 0.1820\n",
      "Epoch:  4 | train loss: 0.1863\n",
      "Epoch:  4 | train loss: 0.1757\n",
      "Epoch:  5 | train loss: 0.1736\n",
      "Epoch:  5 | train loss: 0.1727\n",
      "Epoch:  5 | train loss: 0.1735\n",
      "Epoch:  5 | train loss: 0.1744\n",
      "Epoch:  5 | train loss: 0.1727\n",
      "Epoch:  5 | train loss: 0.1684\n",
      "Epoch:  5 | train loss: 0.1711\n",
      "Epoch:  5 | train loss: 0.1697\n",
      "Epoch:  5 | train loss: 0.1695\n",
      "Epoch:  5 | train loss: 0.1719\n",
      "Epoch:  6 | train loss: 0.1682\n",
      "Epoch:  6 | train loss: 0.1632\n",
      "Epoch:  6 | train loss: 0.1674\n",
      "Epoch:  6 | train loss: 0.1608\n",
      "Epoch:  6 | train loss: 0.1657\n",
      "Epoch:  6 | train loss: 0.1657\n",
      "Epoch:  6 | train loss: 0.1633\n",
      "Epoch:  6 | train loss: 0.1651\n",
      "Epoch:  6 | train loss: 0.1679\n",
      "Epoch:  6 | train loss: 0.1678\n",
      "Epoch:  7 | train loss: 0.1626\n",
      "Epoch:  7 | train loss: 0.1556\n",
      "Epoch:  7 | train loss: 0.1581\n",
      "Epoch:  7 | train loss: 0.1636\n",
      "Epoch:  7 | train loss: 0.1577\n",
      "Epoch:  7 | train loss: 0.1600\n",
      "Epoch:  7 | train loss: 0.1610\n",
      "Epoch:  7 | train loss: 0.1611\n",
      "Epoch:  7 | train loss: 0.1624\n",
      "Epoch:  7 | train loss: 0.1572\n",
      "Epoch:  8 | train loss: 0.1572\n",
      "Epoch:  8 | train loss: 0.1581\n",
      "Epoch:  8 | train loss: 0.1598\n",
      "Epoch:  8 | train loss: 0.1561\n",
      "Epoch:  8 | train loss: 0.1601\n",
      "Epoch:  8 | train loss: 0.1543\n",
      "Epoch:  8 | train loss: 0.1508\n",
      "Epoch:  8 | train loss: 0.1554\n",
      "Epoch:  8 | train loss: 0.1575\n",
      "Epoch:  8 | train loss: 0.1557\n",
      "Epoch:  9 | train loss: 0.1511\n",
      "Epoch:  9 | train loss: 0.1531\n",
      "Epoch:  9 | train loss: 0.1523\n",
      "Epoch:  9 | train loss: 0.1526\n",
      "Epoch:  9 | train loss: 0.1503\n",
      "Epoch:  9 | train loss: 0.1499\n",
      "Epoch:  9 | train loss: 0.1509\n",
      "Epoch:  9 | train loss: 0.1523\n",
      "Epoch:  9 | train loss: 0.1549\n",
      "Epoch:  9 | train loss: 0.1538\n",
      "Epoch:  10 | train loss: 0.1507\n",
      "Epoch:  10 | train loss: 0.1489\n",
      "Epoch:  10 | train loss: 0.1525\n",
      "Epoch:  10 | train loss: 0.1481\n",
      "Epoch:  10 | train loss: 0.1532\n",
      "Epoch:  10 | train loss: 0.1511\n",
      "Epoch:  10 | train loss: 0.1462\n",
      "Epoch:  10 | train loss: 0.1473\n",
      "Epoch:  10 | train loss: 0.1508\n",
      "Epoch:  10 | train loss: 0.1499\n",
      " ################## Starting to learn network parameters:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished learning network parameters:\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished running the network.\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished running the network.\n",
      " ############   STNetRandom     A L G O R I T H M   ################\n",
      "Pre-training:\n",
      " ################## Starting to learn network parameters:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished learning network parameters:\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished running the network.\n",
      "Fine-tuning: \n",
      "Epoch:  1 | train loss: 0.6357\n",
      "Epoch:  1 | train loss: 0.5649\n",
      "Epoch:  1 | train loss: 0.5069\n",
      "Epoch:  1 | train loss: 0.4387\n",
      "Epoch:  1 | train loss: 0.3772\n",
      "Epoch:  1 | train loss: 0.3338\n",
      "Epoch:  1 | train loss: 0.3059\n",
      "Epoch:  1 | train loss: 0.2749\n",
      "Epoch:  1 | train loss: 0.2581\n",
      "Epoch:  1 | train loss: 0.2372\n",
      "Epoch:  2 | train loss: 0.2217\n",
      "Epoch:  2 | train loss: 0.2035\n",
      "Epoch:  2 | train loss: 0.1980\n",
      "Epoch:  2 | train loss: 0.1954\n",
      "Epoch:  2 | train loss: 0.1880\n",
      "Epoch:  2 | train loss: 0.1901\n",
      "Epoch:  2 | train loss: 0.1820\n",
      "Epoch:  2 | train loss: 0.1822\n",
      "Epoch:  2 | train loss: 0.1766\n",
      "Epoch:  2 | train loss: 0.1754\n",
      "Epoch:  3 | train loss: 0.1661\n",
      "Epoch:  3 | train loss: 0.1665\n",
      "Epoch:  3 | train loss: 0.1645\n",
      "Epoch:  3 | train loss: 0.1650\n",
      "Epoch:  3 | train loss: 0.1585\n",
      "Epoch:  3 | train loss: 0.1658\n",
      "Epoch:  3 | train loss: 0.1604\n",
      "Epoch:  3 | train loss: 0.1613\n",
      "Epoch:  3 | train loss: 0.1615\n",
      "Epoch:  3 | train loss: 0.1595\n",
      "Epoch:  4 | train loss: 0.1563\n",
      "Epoch:  4 | train loss: 0.1529\n",
      "Epoch:  4 | train loss: 0.1528\n",
      "Epoch:  4 | train loss: 0.1501\n",
      "Epoch:  4 | train loss: 0.1518\n",
      "Epoch:  4 | train loss: 0.1488\n",
      "Epoch:  4 | train loss: 0.1552\n",
      "Epoch:  4 | train loss: 0.1536\n",
      "Epoch:  4 | train loss: 0.1524\n",
      "Epoch:  4 | train loss: 0.1512\n",
      "Epoch:  5 | train loss: 0.1472\n",
      "Epoch:  5 | train loss: 0.1501\n",
      "Epoch:  5 | train loss: 0.1522\n",
      "Epoch:  5 | train loss: 0.1512\n",
      "Epoch:  5 | train loss: 0.1476\n",
      "Epoch:  5 | train loss: 0.1438\n",
      "Epoch:  5 | train loss: 0.1475\n",
      "Epoch:  5 | train loss: 0.1469\n",
      "Epoch:  5 | train loss: 0.1464\n",
      "Epoch:  5 | train loss: 0.1434\n",
      "Epoch:  6 | train loss: 0.1443\n",
      "Epoch:  6 | train loss: 0.1384\n",
      "Epoch:  6 | train loss: 0.1407\n",
      "Epoch:  6 | train loss: 0.1445\n",
      "Epoch:  6 | train loss: 0.1440\n",
      "Epoch:  6 | train loss: 0.1427\n",
      "Epoch:  6 | train loss: 0.1398\n",
      "Epoch:  6 | train loss: 0.1424\n",
      "Epoch:  6 | train loss: 0.1466\n",
      "Epoch:  6 | train loss: 0.1437\n",
      "Epoch:  7 | train loss: 0.1432\n",
      "Epoch:  7 | train loss: 0.1392\n",
      "Epoch:  7 | train loss: 0.1401\n",
      "Epoch:  7 | train loss: 0.1396\n",
      "Epoch:  7 | train loss: 0.1414\n",
      "Epoch:  7 | train loss: 0.1405\n",
      "Epoch:  7 | train loss: 0.1358\n",
      "Epoch:  7 | train loss: 0.1430\n",
      "Epoch:  7 | train loss: 0.1367\n",
      "Epoch:  7 | train loss: 0.1410\n",
      "Epoch:  8 | train loss: 0.1355\n",
      "Epoch:  8 | train loss: 0.1335\n",
      "Epoch:  8 | train loss: 0.1381\n",
      "Epoch:  8 | train loss: 0.1354\n",
      "Epoch:  8 | train loss: 0.1406\n",
      "Epoch:  8 | train loss: 0.1369\n",
      "Epoch:  8 | train loss: 0.1384\n",
      "Epoch:  8 | train loss: 0.1380\n",
      "Epoch:  8 | train loss: 0.1365\n",
      "Epoch:  8 | train loss: 0.1374\n",
      "Epoch:  9 | train loss: 0.1337\n",
      "Epoch:  9 | train loss: 0.1312\n",
      "Epoch:  9 | train loss: 0.1383\n",
      "Epoch:  9 | train loss: 0.1363\n",
      "Epoch:  9 | train loss: 0.1338\n",
      "Epoch:  9 | train loss: 0.1392\n",
      "Epoch:  9 | train loss: 0.1397\n",
      "Epoch:  9 | train loss: 0.1362\n",
      "Epoch:  9 | train loss: 0.1382\n",
      "Epoch:  9 | train loss: 0.1359\n",
      "Epoch:  10 | train loss: 0.1311\n",
      "Epoch:  10 | train loss: 0.1296\n",
      "Epoch:  10 | train loss: 0.1336\n",
      "Epoch:  10 | train loss: 0.1340\n",
      "Epoch:  10 | train loss: 0.1324\n",
      "Epoch:  10 | train loss: 0.1349\n",
      "Epoch:  10 | train loss: 0.1370\n",
      "Epoch:  10 | train loss: 0.1366\n",
      "Epoch:  10 | train loss: 0.1335\n",
      "Epoch:  10 | train loss: 0.1379\n",
      " ################## Starting to learn network parameters:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished learning network parameters:\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished running the network.\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished running the network.\n",
      " ############   STNetProcrustean     A L G O R I T H M   ################\n",
      "Pre-training:\n",
      " ################## Starting to learn network parameters:  ##################\n",
      "layer-units:\n",
      "** 1 ** - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -** 2 ** - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -** 3 ** - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -** 4 ** - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -** 5 ** - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "Finished learning network parameters:\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished running the network.\n",
      "Fine-tuning: \n",
      "Epoch:  1 | train loss: 0.1782\n",
      "Epoch:  1 | train loss: 0.1850\n",
      "Epoch:  1 | train loss: 0.1722\n",
      "Epoch:  1 | train loss: 0.1761\n",
      "Epoch:  1 | train loss: 0.1738\n",
      "Epoch:  1 | train loss: 0.1707\n",
      "Epoch:  1 | train loss: 0.1697\n",
      "Epoch:  1 | train loss: 0.1636\n",
      "Epoch:  1 | train loss: 0.1662\n",
      "Epoch:  1 | train loss: 0.1645\n",
      "Epoch:  2 | train loss: 0.1615\n",
      "Epoch:  2 | train loss: 0.1585\n",
      "Epoch:  2 | train loss: 0.1558\n",
      "Epoch:  2 | train loss: 0.1536\n",
      "Epoch:  2 | train loss: 0.1570\n",
      "Epoch:  2 | train loss: 0.1565\n",
      "Epoch:  2 | train loss: 0.1566\n",
      "Epoch:  2 | train loss: 0.1565\n",
      "Epoch:  2 | train loss: 0.1488\n",
      "Epoch:  2 | train loss: 0.1559\n",
      "Epoch:  3 | train loss: 0.1536\n",
      "Epoch:  3 | train loss: 0.1456\n",
      "Epoch:  3 | train loss: 0.1508\n",
      "Epoch:  3 | train loss: 0.1491\n",
      "Epoch:  3 | train loss: 0.1526\n",
      "Epoch:  3 | train loss: 0.1475\n",
      "Epoch:  3 | train loss: 0.1473\n",
      "Epoch:  3 | train loss: 0.1483\n",
      "Epoch:  3 | train loss: 0.1464\n",
      "Epoch:  3 | train loss: 0.1452\n",
      "Epoch:  4 | train loss: 0.1464\n",
      "Epoch:  4 | train loss: 0.1408\n",
      "Epoch:  4 | train loss: 0.1383\n",
      "Epoch:  4 | train loss: 0.1466\n",
      "Epoch:  4 | train loss: 0.1398\n",
      "Epoch:  4 | train loss: 0.1403\n",
      "Epoch:  4 | train loss: 0.1375\n",
      "Epoch:  4 | train loss: 0.1411\n",
      "Epoch:  4 | train loss: 0.1389\n",
      "Epoch:  4 | train loss: 0.1423\n",
      "Epoch:  5 | train loss: 0.1375\n",
      "Epoch:  5 | train loss: 0.1410\n",
      "Epoch:  5 | train loss: 0.1395\n",
      "Epoch:  5 | train loss: 0.1404\n",
      "Epoch:  5 | train loss: 0.1395\n",
      "Epoch:  5 | train loss: 0.1414\n",
      "Epoch:  5 | train loss: 0.1351\n",
      "Epoch:  5 | train loss: 0.1386\n",
      "Epoch:  5 | train loss: 0.1420\n",
      "Epoch:  5 | train loss: 0.1377\n",
      "Epoch:  6 | train loss: 0.1337\n",
      "Epoch:  6 | train loss: 0.1346\n",
      "Epoch:  6 | train loss: 0.1369\n",
      "Epoch:  6 | train loss: 0.1357\n",
      "Epoch:  6 | train loss: 0.1392\n",
      "Epoch:  6 | train loss: 0.1332\n",
      "Epoch:  6 | train loss: 0.1309\n",
      "Epoch:  6 | train loss: 0.1388\n",
      "Epoch:  6 | train loss: 0.1334\n",
      "Epoch:  6 | train loss: 0.1346\n",
      "Epoch:  7 | train loss: 0.1297\n",
      "Epoch:  7 | train loss: 0.1288\n",
      "Epoch:  7 | train loss: 0.1324\n",
      "Epoch:  7 | train loss: 0.1337\n",
      "Epoch:  7 | train loss: 0.1334\n",
      "Epoch:  7 | train loss: 0.1271\n",
      "Epoch:  7 | train loss: 0.1306\n",
      "Epoch:  7 | train loss: 0.1305\n",
      "Epoch:  7 | train loss: 0.1313\n",
      "Epoch:  7 | train loss: 0.1352\n",
      "Epoch:  8 | train loss: 0.1286\n",
      "Epoch:  8 | train loss: 0.1253\n",
      "Epoch:  8 | train loss: 0.1280\n",
      "Epoch:  8 | train loss: 0.1282\n",
      "Epoch:  8 | train loss: 0.1318\n",
      "Epoch:  8 | train loss: 0.1277\n",
      "Epoch:  8 | train loss: 0.1286\n",
      "Epoch:  8 | train loss: 0.1314\n",
      "Epoch:  8 | train loss: 0.1275\n",
      "Epoch:  8 | train loss: 0.1333\n",
      "Epoch:  9 | train loss: 0.1246\n",
      "Epoch:  9 | train loss: 0.1226\n",
      "Epoch:  9 | train loss: 0.1253\n",
      "Epoch:  9 | train loss: 0.1240\n",
      "Epoch:  9 | train loss: 0.1216\n",
      "Epoch:  9 | train loss: 0.1273\n",
      "Epoch:  9 | train loss: 0.1270\n",
      "Epoch:  9 | train loss: 0.1292\n",
      "Epoch:  9 | train loss: 0.1322\n",
      "Epoch:  9 | train loss: 0.1288\n",
      "Epoch:  10 | train loss: 0.1229\n",
      "Epoch:  10 | train loss: 0.1265\n",
      "Epoch:  10 | train loss: 0.1205\n",
      "Epoch:  10 | train loss: 0.1259\n",
      "Epoch:  10 | train loss: 0.1263\n",
      "Epoch:  10 | train loss: 0.1278\n",
      "Epoch:  10 | train loss: 0.1228\n",
      "Epoch:  10 | train loss: 0.1270\n",
      "Epoch:  10 | train loss: 0.1266\n",
      "Epoch:  10 | train loss: 0.1261\n",
      " ################## Starting to learn network parameters:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished learning network parameters:\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished running the network.\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **\n",
      "Finished running the network.\n"
     ]
    }
   ],
   "source": [
    "MethodList = ['STNetMLSTC','STNetRandom','STNetProcrustean']\n",
    "#MethodList = ['STNetProcrustean']\n",
    "for RepInd in range(numRep):\n",
    "    print('.......................................  Repetition = ', RepInd + 1)\n",
    "    # F_train, F_test = Tools.data_generator(n, N_train, N_test, 'AR1', SourceParam=Rho)\n",
    "    #         norm_train = np.divide(np.linalg.norm(F_train) ** 2, np.prod(F_train.shape))\n",
    "    #         norm_test = np.divide(np.linalg.norm(F_test) ** 2, np.prod(F_test.shape))\n",
    "    for MethodInd, Method in enumerate(MethodList):\n",
    "        print(' ############   ' + Method + '     A L G O R I T H M   ################')\n",
    "\n",
    "        ###############################################\n",
    "        # Pre-training:\n",
    "        print('Pre-training:')\n",
    "        if 'Random' in Method:\n",
    "            Learner = 'Random'\n",
    "        elif 'MLSTC' in Method:\n",
    "            Learner = 'SuccessivePCA'\n",
    "        elif 'Procrustean' in Method:\n",
    "            Learner = 'ProcrusteanPCA'    \n",
    "        ##    \n",
    "        obj_pretrain = MLSTC.BaseLearner(k, L, m=m, Learner=Learner)\n",
    "        _, _, _ = obj_pretrain.run(F_train)\n",
    "        #\n",
    "        if 'Random' in Method:\n",
    "            exec ('dist_' + 'Random'  + '_train[RepInd,:] = obj_pretrain.distortion/norm_train')\n",
    "            exec ('rate_' + 'Random'  + '[RepInd,:] = obj_pretrain.rate')\n",
    "\n",
    "        elif 'MLSTC' in Method:\n",
    "            exec ('dist_' + 'MLSTC'  + '_train[RepInd,:] = obj_pretrain.distortion/norm_train')\n",
    "            exec ('rate_' + 'MLSTC'  + '[RepInd,:] = obj_pretrain.rate')\n",
    "\n",
    "        elif 'Procrustean' in Method:\n",
    "            exec ('dist_' + 'Procrustean'  + '_train[RepInd,:] = obj_pretrain.distortion/norm_train')\n",
    "            exec ('rate_' + 'Procrustean'  + '[RepInd,:] = obj_pretrain.rate')\n",
    "            \n",
    "        # testing the pre-training:\n",
    "        obj_pretest = MLSTC.fwdPass(obj_pretrain.params, k, ternaryProbMap=obj_pretrain.prob_z)\n",
    "        _, _, _ = obj_pretest.run(F_test)\n",
    "        if 'Random' in Method:\n",
    "            exec ('dist_' + 'Random'  + '_test[RepInd,:] = obj_pretest.distortion/norm_test')\n",
    "        elif 'MLSTC' in Method:\n",
    "            exec ('dist_' + 'MLSTC'  + '_test[RepInd,:] = obj_pretest.distortion/norm_test')\n",
    "        elif 'Procrustean' in Method:\n",
    "            exec ('dist_' + 'Procrustean'  + '_test[RepInd,:] = obj_pretest.distortion/norm_test')    \n",
    "\n",
    "        if 'STNet' not in Method:\n",
    "            continue\n",
    "        ###############################################\n",
    "        # Fine-tuning:\n",
    "        print('Fine-tuning: ')\n",
    "        params = obj_pretrain.params\n",
    "        #############################\n",
    "        ## Training to fine-tune:\n",
    "        obj_bp_train = STNetTorcher.fwdPass(params, k, nlinStrategy='KBest', beta_shape='scalar')\n",
    "        optimizer = torch.optim.Adam(obj_bp_train.parameters(), betas= (0.9, 0.999),lr=learning_rate, weight_decay=weight_decay)\n",
    "        #optimizer = torch.optim.SGD(obj_bp_train.parameters(), lr=learning_rate, momentum=0.1,dampening=0.1, weight_decay=weight_decay)\n",
    "        # ################################\n",
    "        F_torch = Variable(torch.from_numpy(np.copy(F_train)).type(dtype))\n",
    "        counter = 0\n",
    "        for epoch in range(num_epoch):\n",
    "            rand_perm = np.random.permutation(N_train)\n",
    "            batch_map = []\n",
    "            for iter in range(num_batch):\n",
    "                batch_map.append(\n",
    "                    rand_perm[np.arange(iter * int(N_train / num_batch), (iter + 1) * int(N_train / num_batch))])\n",
    "            for iter in range(num_batch):  # gives batch data\n",
    "                batch_map_current = batch_map[iter]\n",
    "                F_minibatch = F_torch[:, batch_map_current]\n",
    "                F_chapeau = obj_bp_train.forward(F_minibatch)\n",
    "\n",
    "                loss = (F_minibatch - F_chapeau).pow(2).mean()\n",
    "                if iter % 10 == 0:\n",
    "                    print('Epoch: ', epoch + 1, '| train loss: %.4f' % loss.data[0])\n",
    "                # Zero gradients, perform a backward pass, and update the weights.\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                counter += 1\n",
    "                # -------------\n",
    "        for l in range(L):\n",
    "            obj_bp_train.params[l]['A'] /= np.linalg.norm(obj_bp_train.params[l]['A'], axis=1).reshape(-1,\n",
    "                                                                                                       1)\n",
    "\n",
    "        # Re-adjusting beta:\n",
    "        obj_beta = MLSTC.BaseLearner(k, L, m=m, Learner='SuccessivePCA')\n",
    "        obj_beta.reRun_reweightUpdate(F_train, obj_bp_train.params)\n",
    "        ###### Re-running on the final network on train set:\n",
    "        obj_beta_train = MLSTC.fwdPass(obj_beta.params, k)\n",
    "        _ = obj_beta_train.run(F_train)\n",
    "        exec ('dist_' + Method  + '_train[RepInd,:] = obj_beta_train.distortion/norm_train')\n",
    "        exec ('rate_' + Method  + '[RepInd,:] = obj_beta_train.rate')\n",
    "        ###### Re-running on the final network on test set:\n",
    "        obj_beta_test = MLSTC.fwdPass(obj_beta.params, k)\n",
    "        _ = obj_beta_test.run(F_test)\n",
    "        exec ('dist_' + Method  + '_test[RepInd,:] = obj_beta_test.distortion/norm_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MethodList = ['STNetRandom','STNetMLSTC','Random','MLSTC','STNetProcrustean','Procrustean']\n",
    "#MethodList = ['STNetProcrustean','Procrustean']\n",
    "curve_dict = {}\n",
    "for MethodInd,Method in enumerate(MethodList):\n",
    "    exec('x = n* np.mean(rate_' +  Method  + ',axis=0)')\n",
    "    exec('y = np.mean(dist_' +  Method  + '_train,axis=0)')\n",
    "    exec(\"curve_dict['n\" + str(n)+ '_m' + str(m) + '_N' + str(N_train) +\"_DBit_\"+ Method + \"_train'] = (x,y)\") \n",
    "    exec('y = np.mean(dist_' +  Method  + '_test,axis=0)')\n",
    "    exec(\"curve_dict['n\" + str(n)+ '_m' + str(m) + '_N' + str(N_train) +\"_DBit_\"+ Method + \"_test'] = (x,y)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST_BP_n784_m500_N60000_DBit_STNetRandom_train.dat\n",
      "MNIST_BP_n784_m500_N60000_DBit_STNetRandom_test.dat\n",
      "MNIST_BP_n784_m500_N60000_DBit_STNetMLSTC_train.dat\n",
      "MNIST_BP_n784_m500_N60000_DBit_STNetMLSTC_test.dat\n",
      "MNIST_BP_n784_m500_N60000_DBit_Random_train.dat\n",
      "MNIST_BP_n784_m500_N60000_DBit_Random_test.dat\n",
      "MNIST_BP_n784_m500_N60000_DBit_MLSTC_train.dat\n",
      "MNIST_BP_n784_m500_N60000_DBit_MLSTC_test.dat\n",
      "MNIST_BP_n784_m500_N60000_DBit_STNetProcrustean_train.dat\n",
      "MNIST_BP_n784_m500_N60000_DBit_STNetProcrustean_test.dat\n",
      "MNIST_BP_n784_m500_N60000_DBit_Procrustean_train.dat\n",
      "MNIST_BP_n784_m500_N60000_DBit_Procrustean_test.dat\n"
     ]
    }
   ],
   "source": [
    "# Saving the results for PGFplott:\n",
    "PGF_path = '/home/sssohrab/Dropbox/Thesis/Thesis/Chapter4/Pics/PGF/dat/BP/'\n",
    "ExpName = Database + '_BP'\n",
    "#\n",
    "for curve_name, curve_xy in curve_dict.items():\n",
    "    fname = ExpName + '_' + curve_name + '.dat'\n",
    "    print(fname)\n",
    "    x = curve_xy[0].astype(float).reshape(-1)\n",
    "    y = curve_xy[1].astype(float).reshape(-1)\n",
    "    np.savetxt(PGF_path + fname, np.transpose([x,y]), fmt='%8f', delimiter='   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5f42320780>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rate_MLSTC.T,dist_MLSTC_test.T)\n",
    "plt.plot(rate_STNetRandom.T,dist_STNetRandom_test.T)\n",
    "plt.plot(rate_STNetMLSTC.T,dist_STNetMLSTC_test.T)\n",
    "plt.plot(rate_STNetProcrustean.T,dist_STNetProcrustean_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
