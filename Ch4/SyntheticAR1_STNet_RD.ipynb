{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sparse_ternary_networks.Tools as T\n",
    "import sparse_ternary_networks.loadLab as load\n",
    "import numpy as np\n",
    "import sparse_ternary_networks.MLSTC as MLSTC\n",
    "import matplotlib.pyplot as plt\n",
    "##\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import sparse_ternary_networks.PyTorcher_MLSTC as STNetTorcher\n",
    "import RRQ.Tools as Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the parameters:\n",
    "n = 512      # dimension\n",
    "m = 512       # dimension of the codes\n",
    "N_train = 10000     # number of samples\n",
    "N_test = 10000     # number of samples\n",
    "k = 2         # number of non-zero elements of the STC at each stage.\n",
    "L = 10        # number of layer units of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch hyper-params:\n",
    "dtype = torch.FloatTensor\n",
    "num_epoch = 10    \n",
    "learning_rate = 1e-3  \n",
    "num_batch = 100\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numRep = 1\n",
    "RhoList = [0.5]\n",
    "#RhoList = [0,0.5,0.99]\n",
    "MethodList = ['STNetRandom','STNetMLSTC','Random','MLSTC']\n",
    "#MethodList = ['RQ']\n",
    "for RhoInd,Rho in enumerate(RhoList):\n",
    "    for Method in MethodList:\n",
    "        exec('dist_' + Method + '_Rho' + str(int(Rho*100)) + '_train = np.zeros((numRep,L+1))')\n",
    "        exec('dist_' + Method + '_Rho' + str(int(Rho*100)) + '_test = np.zeros((numRep,L+1))')\n",
    "        #\n",
    "        exec('rate_' + Method + '_Rho' + str(int(Rho*100)) + ' = np.zeros((numRep,L+1))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................  Repetition =  1\n",
      "....................  Rho =  0.5\n",
      " ############   STNetMLSTC     A L G O R I T H M   ################\n",
      "Pre-training:\n",
      " ################## Starting to learn network parameters:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **** 6 **** 7 **** 8 **** 9 **** 10 **\n",
      "Finished learning network parameters:\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **** 6 **** 7 **** 8 **** 9 **** 10 **\n",
      "Finished running the network.\n",
      "Fine-tuning: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sssohrab/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:69: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | train loss: 0.5636\n",
      "Epoch:  1 | train loss: 0.5621\n",
      "Epoch:  1 | train loss: 0.5708\n",
      "Epoch:  1 | train loss: 0.5697\n",
      "Epoch:  1 | train loss: 0.5691\n",
      "Epoch:  1 | train loss: 0.5659\n",
      "Epoch:  1 | train loss: 0.5716\n",
      "Epoch:  1 | train loss: 0.5714\n",
      "Epoch:  1 | train loss: 0.5706\n",
      "Epoch:  1 | train loss: 0.5746\n",
      "Epoch:  2 | train loss: 0.5185\n",
      "Epoch:  2 | train loss: 0.5087\n",
      "Epoch:  2 | train loss: 0.5153\n",
      "Epoch:  2 | train loss: 0.5187\n",
      "Epoch:  2 | train loss: 0.5251\n",
      "Epoch:  2 | train loss: 0.5283\n",
      "Epoch:  2 | train loss: 0.5216\n",
      "Epoch:  2 | train loss: 0.5241\n",
      "Epoch:  2 | train loss: 0.5347\n",
      "Epoch:  2 | train loss: 0.5381\n",
      "Epoch:  3 | train loss: 0.4750\n",
      "Epoch:  3 | train loss: 0.4712\n",
      "Epoch:  3 | train loss: 0.4780\n",
      "Epoch:  3 | train loss: 0.4930\n",
      "Epoch:  3 | train loss: 0.4962\n",
      "Epoch:  3 | train loss: 0.4913\n",
      "Epoch:  3 | train loss: 0.5063\n",
      "Epoch:  3 | train loss: 0.4928\n",
      "Epoch:  3 | train loss: 0.5128\n",
      "Epoch:  3 | train loss: 0.5057\n",
      "Epoch:  4 | train loss: 0.4391\n",
      "Epoch:  4 | train loss: 0.4413\n",
      "Epoch:  4 | train loss: 0.4420\n",
      "Epoch:  4 | train loss: 0.4589\n",
      "Epoch:  4 | train loss: 0.4630\n",
      "Epoch:  4 | train loss: 0.4755\n",
      "Epoch:  4 | train loss: 0.4643\n",
      "Epoch:  4 | train loss: 0.4748\n",
      "Epoch:  4 | train loss: 0.4814\n",
      "Epoch:  4 | train loss: 0.4830\n",
      "Epoch:  5 | train loss: 0.3978\n",
      "Epoch:  5 | train loss: 0.4017\n",
      "Epoch:  5 | train loss: 0.4260\n",
      "Epoch:  5 | train loss: 0.4308\n",
      "Epoch:  5 | train loss: 0.4210\n",
      "Epoch:  5 | train loss: 0.4442\n",
      "Epoch:  5 | train loss: 0.4605\n",
      "Epoch:  5 | train loss: 0.4502\n",
      "Epoch:  5 | train loss: 0.4532\n",
      "Epoch:  5 | train loss: 0.4671\n",
      "Epoch:  6 | train loss: 0.3872\n",
      "Epoch:  6 | train loss: 0.3846\n",
      "Epoch:  6 | train loss: 0.3991\n",
      "Epoch:  6 | train loss: 0.4120\n",
      "Epoch:  6 | train loss: 0.4100\n",
      "Epoch:  6 | train loss: 0.4141\n",
      "Epoch:  6 | train loss: 0.4284\n",
      "Epoch:  6 | train loss: 0.4435\n",
      "Epoch:  6 | train loss: 0.4414\n",
      "Epoch:  6 | train loss: 0.4396\n",
      "Epoch:  7 | train loss: 0.3767\n",
      "Epoch:  7 | train loss: 0.3763\n",
      "Epoch:  7 | train loss: 0.3831\n",
      "Epoch:  7 | train loss: 0.3853\n",
      "Epoch:  7 | train loss: 0.4106\n",
      "Epoch:  7 | train loss: 0.4090\n",
      "Epoch:  7 | train loss: 0.4131\n",
      "Epoch:  7 | train loss: 0.4205\n",
      "Epoch:  7 | train loss: 0.4243\n",
      "Epoch:  7 | train loss: 0.4265\n",
      "Epoch:  8 | train loss: 0.3707\n",
      "Epoch:  8 | train loss: 0.3696\n",
      "Epoch:  8 | train loss: 0.3826\n",
      "Epoch:  8 | train loss: 0.3851\n",
      "Epoch:  8 | train loss: 0.3935\n",
      "Epoch:  8 | train loss: 0.3911\n",
      "Epoch:  8 | train loss: 0.4046\n",
      "Epoch:  8 | train loss: 0.4059\n",
      "Epoch:  8 | train loss: 0.4158\n",
      "Epoch:  8 | train loss: 0.4014\n",
      "Epoch:  9 | train loss: 0.3558\n",
      "Epoch:  9 | train loss: 0.3584\n",
      "Epoch:  9 | train loss: 0.3615\n",
      "Epoch:  9 | train loss: 0.3787\n",
      "Epoch:  9 | train loss: 0.3682\n",
      "Epoch:  9 | train loss: 0.3765\n",
      "Epoch:  9 | train loss: 0.3904\n",
      "Epoch:  9 | train loss: 0.4100\n",
      "Epoch:  9 | train loss: 0.4046\n",
      "Epoch:  9 | train loss: 0.3977\n",
      "Epoch:  10 | train loss: 0.3392\n",
      "Epoch:  10 | train loss: 0.3458\n",
      "Epoch:  10 | train loss: 0.3492\n",
      "Epoch:  10 | train loss: 0.3698\n",
      "Epoch:  10 | train loss: 0.3746\n",
      "Epoch:  10 | train loss: 0.3827\n",
      "Epoch:  10 | train loss: 0.3899\n",
      "Epoch:  10 | train loss: 0.4062\n",
      "Epoch:  10 | train loss: 0.4033\n",
      "Epoch:  10 | train loss: 0.3951\n",
      " ################## Starting to learn network parameters:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **** 6 **** 7 **** 8 **** 9 **** 10 **\n",
      "Finished learning network parameters:\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **** 6 **** 7 **** 8 **** 9 **** 10 **\n",
      "Finished running the network.\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **** 6 **** 7 **** 8 **** 9 **** 10 **\n",
      "Finished running the network.\n",
      " ############   MLSTC     A L G O R I T H M   ################\n",
      "Pre-training:\n",
      " ################## Starting to learn network parameters:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **** 6 **** 7 **** 8 **** 9 **** 10 **\n",
      "Finished learning network parameters:\n",
      " Running the network:\n",
      " ##################  Running the network:  ##################\n",
      "layer-units:\n",
      "** 1 **** 2 **** 3 **** 4 **** 5 **** 6 **** 7 **** 8 **** 9 **** 10 **\n",
      "Finished running the network.\n"
     ]
    }
   ],
   "source": [
    "MethodList = ['STNetMLSTC','MLSTC']\n",
    "for RepInd in range(numRep):\n",
    "    print('.......................................  Repetition = ', RepInd + 1)\n",
    "    for RhoInd, Rho in enumerate(RhoList):\n",
    "        print('....................  Rho = ', Rho)\n",
    "        F_train, F_test = Tools.data_generator(n, N_train, N_test, 'AR1', SourceParam=Rho)\n",
    "        norm_train = np.divide(np.linalg.norm(F_train) ** 2, np.prod(F_train.shape))\n",
    "        norm_test = np.divide(np.linalg.norm(F_test) ** 2, np.prod(F_test.shape))\n",
    "        for MethodInd, Method in enumerate(MethodList):\n",
    "            print(' ############   ' + Method + '     A L G O R I T H M   ################')\n",
    "\n",
    "            ###############################################\n",
    "            # Pre-training:\n",
    "            print('Pre-training:')\n",
    "            if 'Random' in Method:\n",
    "                Learner = 'Random'\n",
    "            elif 'MLSTC' in Method:\n",
    "                Learner = 'SuccessivePCA'\n",
    "            ##    \n",
    "            obj_pretrain = MLSTC.BaseLearner(k, L, m=m, Learner=Learner)\n",
    "            _, _, _ = obj_pretrain.run(F_train)\n",
    "            #\n",
    "            if 'Random' in Method:\n",
    "                exec('dist_' + 'Random' + '_Rho' + str(\n",
    "                    int(Rho * 100)) + '_train[RepInd,:] = obj_pretrain.distortion/norm_train')\n",
    "                exec('rate_' + 'Random' + '_Rho' + str(int(Rho * 100)) + '[RepInd,:] = obj_pretrain.rate')\n",
    "\n",
    "            elif 'MLSTC' in Method:\n",
    "                exec('dist_' + 'MLSTC' + '_Rho' + str(\n",
    "                    int(Rho * 100)) + '_train[RepInd,:] = obj_pretrain.distortion/norm_train')\n",
    "                exec('rate_' + 'MLSTC' + '_Rho' + str(int(Rho * 100)) + '[RepInd,:] = obj_pretrain.rate')\n",
    "\n",
    "            # testing the pre-training:\n",
    "            obj_pretest = MLSTC.fwdPass(obj_pretrain.params, k, ternaryProbMap=obj_pretrain.prob_z)\n",
    "            _, _, _ = obj_pretest.run(F_test)\n",
    "            if 'Random' in Method:\n",
    "                exec('dist_' + 'Random' + '_Rho' + str(\n",
    "                    int(Rho * 100)) + '_test[RepInd,:] = obj_pretest.distortion/norm_test')\n",
    "            elif 'MLSTC' in Method:\n",
    "                exec('dist_' + 'MLSTC' + '_Rho' + str(\n",
    "                    int(Rho * 100)) + '_test[RepInd,:] = obj_pretest.distortion/norm_test')\n",
    "\n",
    "            if 'STNet' not in Method:\n",
    "                continue\n",
    "            ###############################################\n",
    "            # Fine-tuning:\n",
    "            print('Fine-tuning: ')\n",
    "            params = obj_pretrain.params\n",
    "            #############################\n",
    "            ## Training to fine-tune:\n",
    "            obj_bp_train = STNetTorcher.fwdPass(params, k, nlinStrategy='KBest', beta_shape='scalar')\n",
    "            optimizer = torch.optim.Adam(obj_bp_train.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            # ################################\n",
    "            F_torch = Variable(torch.from_numpy(np.copy(F_train)).type(dtype))\n",
    "            counter = 0\n",
    "            for epoch in range(num_epoch):\n",
    "                rand_perm = np.random.permutation(N_train)\n",
    "                batch_map = []\n",
    "                for iter in range(num_batch):\n",
    "                    batch_map.append(\n",
    "                        rand_perm[np.arange(iter * int(N_train / num_batch), (iter + 1) * int(N_train / num_batch))])\n",
    "                for iter in range(num_batch):  # gives batch data\n",
    "                    batch_map_current = batch_map[iter]\n",
    "                    F_minibatch = F_torch[:, batch_map_current]\n",
    "                    F_chapeau = obj_bp_train.forward(F_minibatch)\n",
    "\n",
    "                    loss = (F_minibatch - F_chapeau).pow(2).mean()\n",
    "                    if iter % 10 == 0:\n",
    "                        print('Epoch: ', epoch + 1, '| train loss: %.4f' % loss.data[0])\n",
    "                    # Zero gradients, perform a backward pass, and update the weights.\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    counter += 1\n",
    "                    # -------------\n",
    "            for l in range(L):\n",
    "                obj_bp_train.params[l]['A'] /= np.linalg.norm(obj_bp_train.params[l]['A'], axis=1).reshape(-1,\n",
    "                                                                                                           1)\n",
    "\n",
    "            # Re-adjusting beta:\n",
    "            obj_beta = MLSTC.BaseLearner(k, L, m=m, Learner='SuccessivePCA')\n",
    "            obj_beta.reRun_reweightUpdate(F_train, obj_bp_train.params)\n",
    "            ###### Re-running on the final network on train set:\n",
    "            obj_beta_train = MLSTC.fwdPass(obj_beta.params, k)\n",
    "            _ = obj_beta_train.run(F_train)\n",
    "            exec('dist_' + Method + '_Rho' + str(\n",
    "                int(Rho * 100)) + '_train[RepInd,:] = obj_beta_train.distortion/norm_train')\n",
    "            exec('rate_' + Method + '_Rho' + str(int(Rho * 100)) + '[RepInd,:] = obj_beta_train.rate')\n",
    "            ###### Re-running on the final network on test set:\n",
    "            obj_beta_test = MLSTC.fwdPass(obj_beta.params, k)\n",
    "            _ = obj_beta_test.run(F_test)\n",
    "            exec('dist_' + Method + '_Rho' + str(\n",
    "                int(Rho * 100)) + '_test[RepInd,:] = obj_beta_test.distortion/norm_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_dict = {}\n",
    "for RhoInd,Rho in enumerate(RhoList):\n",
    "    for MethodInd,Method in enumerate(MethodList):\n",
    "        exec('x = np.mean(rate_' +  Method + '_Rho' + str(int(Rho*100)) + ',axis=0)')\n",
    "        exec('y = np.mean(dist_' +  Method + '_Rho' + str(int(Rho*100)) + '_train,axis=0)')\n",
    "        exec(\"curve_dict['Rho\" + str(int(Rho*100))+ '_n' + str(n)+ '_m' + str(m) +\"_DR_\"+ Method + \"_train'] = (x,y)\") \n",
    "        exec('y = np.mean(dist_' +  Method + '_Rho' + str(int(Rho*100)) + '_test,axis=0)')\n",
    "        exec(\"curve_dict['Rho\" + str(int(Rho*100))+ '_n' + str(n)+ '_m' + str(m) +\"_DR_\"+ Method + \"_test'] = (x,y)\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the results for PGFplott:\n",
    "# PGF_path = 'PGF/dat/BP/'\n",
    "# ExpName = 'SyntheticAR1_BP'\n",
    "# #\n",
    "# for curve_name, curve_xy in curve_dict.items():\n",
    "#     fname = ExpName + '_' + curve_name + '.dat'\n",
    "#     print(fname)\n",
    "#     x = curve_xy[0].astype(float).reshape(-1)\n",
    "#     y = curve_xy[1].astype(float).reshape(-1)\n",
    "#     np.savetxt(PGF_path + fname, np.transpose([x,y]), fmt='%8f', delimiter='   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
